{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c150e8c8-dfba-4a83-8720-aec90058d953",
   "metadata": {},
   "source": [
    "Nowadays, many server applications are not installed and run directly on physical hosts or virtual machines any more. Instead, application code is often built into container images, and run in so-called *pods* in a Kubernetes cluster. Kubernetes provides a standardized way to orchestrate applications and works the same way no matter where the cluster is running.\n",
    "\n",
    "Kubernetes clusters are often hosted and managed by cloud providers. They can also be deployed on-premise though, and either be created and managed by a service provider, or with tools like [Kubespray](https://github.com/kubernetes-sigs/kubespray). Most clusters are long-lived, and consist of multiple nodes for redundancy.\n",
    "\n",
    "However, sometimes a cluster that can be created and discarded quickly and easily on a single computer can be very useful:\n",
    "* A developer might want to create a cluster on their development machine for playing around with Kubernetes, exploring the newest tools, or testing their newly developed code and Kubernetes resources. If they used a cluster that is shared with others or even with production workloads instead, they might get into the way of others, or worse, break things that should better not break.\n",
    "* Another use case is automated testing of application deployments in Kubernetes, or testing of applications that interact with Kubernetes resources themselves. This works best in a dedicated cluster that is set up in a clean state just for this purpose, and thrown away after the tests.\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "There are a few solutions for setting up a local Kubernetes cluster. I choose [*k3d*](https://k3d.io/) here because it is pretty easy to use - it is a single binary that requires only Docker to create its cluster in a container. Unlike [*kind*](https://kind.sigs.k8s.io/), which I also like a lot and which follows the same approach, it has an ingress controller built in. This makes accessing the applications that run in the cluster easier, and allows testing of ingress resources. With kind, this can also be done, but only after an ingress controller has been installed.\n",
    "\n",
    "This post shows how to set up a cluster with k3d, and deploy a self-made application. It does not assume any prior Kubernetes knowledge. If you have some Kubernetes experience, and you are interested in playing around with k3d, feel free to skip most of the text and just look at the commands ðŸ™‚\n",
    "You can also download the [Jupyter notebook](index.ipynb) that this post is based on, and play around with it.\n",
    "\n",
    "I use Linux to work with k3d and Kubernetes in general, but most of what I do should work pretty much the same way on a Mac. Trying it on Windows might require more modifictions though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9147d07-bb01-4fa5-a66c-614e53f35c2b",
   "metadata": {},
   "source": [
    "## Installing k3d and kubectl\n",
    "We will need not only k3d, but also *kubectl*, which is the command-line interface for interacting with Kubernetes clusters.\n",
    "\n",
    "There is a range of installation options. I prefer using [*arkade*](https://github.com/alexellis/arkade), which allows to install many tools that are related to Kubernetes easily.\n",
    "\n",
    "We will use the arkade installation script as [recommended in its README](https://github.com/alexellis/arkade#getting-arkade), but run it without root permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67637a76-b064-4e0c-b1ae-96a4ca7417aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x86_64\n",
      "Downloading package https://github.com/alexellis/arkade/releases/download/0.8.28/arkade as /home/frank/code/github/freininghaus/freininghaus.github.io/posts/2022-07-12-local-k8s-cluster-with-k3d/arkade\n",
      "Download complete.\n",
      "\n",
      "============================================================\n",
      "  The script was run as a user who is unable to write\n",
      "  to /usr/local/bin. To complete the installation the\n",
      "  following commands may need to be run manually.\n",
      "============================================================\n",
      "\n",
      "  sudo cp arkade /usr/local/bin/arkade\n",
      "  sudo ln -sf /usr/local/bin/arkade /usr/local/bin/ark\n",
      "\n"
     ]
    }
   ],
   "source": [
    "curl -sLS https://get.arkade.dev | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c391b-ec5b-495b-abfd-c15c26f73d87",
   "metadata": {},
   "source": [
    "We could now move the `arkade` binary to a directory in our `PATH`, or re-run the download script with root permissions. But we can also run the binary from the current directory and use it to install the tools that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5acc5de-58a6-4a38-b0a2-43fb22c09207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.73 MiB / 44.73 MiB [------------------------------------------------] 100.00%\n",
      "2022/07/12 13:56:22 Copying /tmp/kubectl to /home/frank/.arkade/bin/kubectl\n",
      "2022/07/12 13:56:23 Looking up version for k3d\n",
      "2022/07/12 13:56:23 Found: v5.4.4\n",
      "16.77 MiB / 16.77 MiB [------------------------------------------------] 100.00%\n",
      "2022/07/12 13:56:24 Looking up version for k3d\n",
      "2022/07/12 13:56:24 Found: v5.4.4\n",
      "2022/07/12 13:56:24 Copying /tmp/k3d-linux-amd64 to /home/frank/.arkade/bin/k3d\n",
      "2022/07/12 13:56:24 Looking up version for jq\n",
      "2022/07/12 13:56:24 Found: jq-1.6\n",
      "3.77 MiB / 3.77 MiB [--------------------------------------------------] 100.00%\n",
      "2022/07/12 13:56:24 Looking up version for jq\n",
      "2022/07/12 13:56:24 Found: jq-1.6\n",
      "2022/07/12 13:56:24 Copying /tmp/jq-linux64 to /home/frank/.arkade/bin/jq\n"
     ]
    }
   ],
   "source": [
    "./arkade get kubectl k3d jq > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f4c25-6390-4c47-a8de-d704ac06739d",
   "metadata": {},
   "source": [
    "We will add the directory where `arkade` puts the downloaded tools to the `PATH` for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add42dd6-13e3-4ccc-b4a8-3682dd926926",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PATH=$PATH:$HOME/.arkade/bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f147ab31-a3b5-413d-93f4-8b89b8ee323c",
   "metadata": {},
   "source": [
    "## Creating a local Kubernetes cluster with k3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c83897-c989-43a7-aa3f-0ef26dc0b462",
   "metadata": {},
   "source": [
    "Now that we have downloaded k3d, we can use it to create our first local Kubernetes cluster. We will give it the name `test-cluster` and forward port 80 on the host to port 80 in the container which matches the node filter \"loadbalancer\". This will enable easy HTTP/HTTPS access to apps running in the cluster via ingress routes, as we will see in the next post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e066908-5573-41b1-b1ae-cfb2429f5ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0000] portmapping '80:80' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy] \n",
      "\u001b[36mINFO\u001b[0m[0000] Prep: Network                                \n",
      "\u001b[36mINFO\u001b[0m[0000] Created network 'k3d-test-cluster'           \n",
      "\u001b[36mINFO\u001b[0m[0000] Created image volume k3d-test-cluster-images \n",
      "\u001b[36mINFO\u001b[0m[0000] Starting new tools node...                   \n",
      "\u001b[36mINFO\u001b[0m[0000] Starting Node 'k3d-test-cluster-tools'       \n",
      "\u001b[36mINFO\u001b[0m[0001] Creating node 'k3d-test-cluster-server-0'    \n",
      "\u001b[36mINFO\u001b[0m[0001] Creating LoadBalancer 'k3d-test-cluster-serverlb' \n",
      "\u001b[36mINFO\u001b[0m[0001] Using the k3d-tools node to gather environment information \n",
      "\u001b[36mINFO\u001b[0m[0002] HostIP: using network gateway 172.31.0.1 address \n",
      "\u001b[36mINFO\u001b[0m[0002] Starting cluster 'test-cluster'              \n",
      "\u001b[36mINFO\u001b[0m[0002] Starting servers...                          \n",
      "\u001b[36mINFO\u001b[0m[0002] Starting Node 'k3d-test-cluster-server-0'    \n",
      "\u001b[36mINFO\u001b[0m[0009] All agents already running.                  \n",
      "\u001b[36mINFO\u001b[0m[0009] Starting helpers...                          \n",
      "\u001b[36mINFO\u001b[0m[0009] Starting Node 'k3d-test-cluster-serverlb'    \n",
      "\u001b[36mINFO\u001b[0m[0017] Injecting records for hostAliases (incl. host.k3d.internal) and for 2 network members into CoreDNS configmap... \n",
      "\u001b[36mINFO\u001b[0m[0019] Cluster 'test-cluster' created successfully! \n",
      "\u001b[36mINFO\u001b[0m[0019] You can now use it like this:                \n",
      "kubectl cluster-info\n"
     ]
    }
   ],
   "source": [
    "k3d cluster create test-cluster -p \"80:80@loadbalancer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d30005-20e5-4253-afd5-e7716f56de0e",
   "metadata": {},
   "source": [
    "We can verify that our cluster is running and can be accessed via `kubectl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0454a2-3cb5-4729-9a60-a1c73a488d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mKubernetes control plane\u001b[0m is running at \u001b[0;33mhttps://0.0.0.0:39309\u001b[0m\n",
      "\u001b[0;32mCoreDNS\u001b[0m is running at \u001b[0;33mhttps://0.0.0.0:39309/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\u001b[0m\n",
      "\u001b[0;32mMetrics-server\u001b[0m is running at \u001b[0;33mhttps://0.0.0.0:39309/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy\u001b[0m\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
     ]
    }
   ],
   "source": [
    "kubectl cluster-info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b9119-9a13-47a2-ad6f-de94759318ed",
   "metadata": {},
   "source": [
    "The cluster has a single node:[<sup id=\"fnref:k3d-multiple-nodes\">1</sup>](#fn:k3d-multiple-nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165ae326-9c58-4465-b214-3ec8f255149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        STATUS     ROLES                  AGE   VERSION\n",
      "k3d-test-cluster-server-0   NotReady   control-plane,master   6s    v1.23.8+k3s1\n"
     ]
    }
   ],
   "source": [
    "kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08fdad-6c17-48f6-8eea-e23577f64088",
   "metadata": {},
   "source": [
    "Note that the status of the node is `NotReady`. We can either wait for a few seconds, or use `kubectl wait`, which runs until the given condition is met:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2be63d-aed3-47c4-b512-f1b954e0cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node/k3d-test-cluster-server-0 condition met\n"
     ]
    }
   ],
   "source": [
    "kubectl wait --for=condition=ready node k3d-test-cluster-server-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2db7c93-b806-44da-b6be-a5ebfbbde86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        STATUS   ROLES                  AGE   VERSION\n",
      "k3d-test-cluster-server-0   Ready    control-plane,master   9s    v1.23.8+k3s1\n"
     ]
    }
   ],
   "source": [
    "kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737661a",
   "metadata": {},
   "source": [
    "## Create a simple web service\n",
    "Let's build a simple application that we will deploy and test in our cluster. We will do it in Python with [FastAPI](https://fastapi.tiangolo.com/) (but we could just as well use any other programming language and framework, of course).\n",
    "\n",
    "The app accepts GET requests at the endpoint `/greet/{name}`, where a name can be substituted for `name`. It responds with a small JSON object that contains a greeting, and also the host name. This will be interesting later on when multiple instances of the application are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a69e705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\n",
      "\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msocket\u001b[39;00m\n",
      "\n",
      "app \u001b[38;5;241m=\u001b[39m fastapi\u001b[38;5;241m.\u001b[39mFastAPI()\n",
      "\n",
      "\n",
      "\u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/greet/\u001b[39m\u001b[38;5;132;01m{name}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreet\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m):\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m {\n",
      "        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n",
      "            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "        },\n",
      "        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n",
      "            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhostname\u001b[39m\u001b[38;5;124m\"\u001b[39m: socket\u001b[38;5;241m.\u001b[39mgethostname()\n",
      "        }\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "# I like to use https://pygments.org/ for syntax highlighting\n",
    "alias cat=pygmentize\n",
    "\n",
    "cat hello-server/hello-app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446cc838",
   "metadata": {},
   "source": [
    "## Build Docker image\n",
    "The `Dockerfile` is quite simple. We just have to copy the Python file into a Python base image and install the dependencies `fastapi` and `uvicorn`. The latter is the web server that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51181758-65d8-4157-807f-d762cf9ea597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;28;01mFROM\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124mpython:3.10-slim\u001b[39m\n",
      "\n",
      "\u001b[38;5;28;01mCOPY\u001b[39;00m\u001b[38;5;250m \u001b[39mhello-app.py ./\n",
      "\n",
      "\u001b[38;5;28;01mRUN\u001b[39;00m\u001b[38;5;250m \u001b[39mpip install fastapi uvicorn\u001b[38;5;241m[\u001b[39mstandard\u001b[38;5;241m]\u001b[39m >/dev/null \u001b[38;5;241m2\u001b[39m>&\u001b[38;5;241m1\u001b[39m\n",
      "\n",
      "\u001b[38;5;28;01mCMD\u001b[39;00m\u001b[38;5;250m \u001b[39muvicorn hello-app:app --host \u001b[38;5;241m0\u001b[39m.0.0.0\n"
     ]
    }
   ],
   "source": [
    "cat hello-server/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc682827-20d4-4e7b-b098-0249a576ef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/4 : FROM python:3.10-slim\n",
      " ---> 24aa51b1b3e9\n",
      "Step 2/4 : COPY hello-app.py ./\n",
      " ---> 11c437b4a570\n",
      "Step 3/4 : RUN pip install fastapi uvicorn[standard] >/dev/null 2>&1\n",
      " ---> Running in f79cefe640e2\n",
      "Removing intermediate container f79cefe640e2\n",
      " ---> 5b57681458aa\n",
      "Step 4/4 : CMD uvicorn hello-app:app --host 0.0.0.0\n",
      " ---> Running in 51940227c7ac\n",
      "Removing intermediate container 51940227c7ac\n",
      " ---> c1b8d149b4a9\n",
      "Successfully built c1b8d149b4a9\n",
      "Successfully tagged my-hello-server:latest\n"
     ]
    }
   ],
   "source": [
    "docker build hello-server/ -t my-hello-server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8ebd0",
   "metadata": {},
   "source": [
    "## Import the Docker image into the Kubernetes cluster\n",
    "To run our service in the Kubernetes cluster, the nodes in the cluster need access to the image. This can be achieved in several ways: we could either\n",
    "* push the image to a public registry,\n",
    "* push the image to a private registry, and configure the cluster such that it has access to this registry, or\n",
    "* load the image to the nodes in the cluster directly.\n",
    "\n",
    "Usually, the third option is the easiest when we run tests and experiments on the local development machine, so we will use it here.[<sup id=\"fnref:local-image-caveat\">2</sup>](#fn:local-image-caveat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "873d793a-d19f-4762-aeab-0bef365d3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0000] Importing image(s) into cluster 'test-cluster' \n",
      "\u001b[36mINFO\u001b[0m[0000] Starting new tools node...                   \n",
      "\u001b[36mINFO\u001b[0m[0000] Starting Node 'k3d-test-cluster-tools'       \n",
      "\u001b[36mINFO\u001b[0m[0001] Saving 1 image(s) from runtime...            \n",
      "\u001b[36mINFO\u001b[0m[0005] Importing images into nodes...               \n",
      "\u001b[36mINFO\u001b[0m[0005] Importing images from tarball '/k3d/images/k3d-test-cluster-images-20220712134140.tar' into node 'k3d-test-cluster-server-0'... \n",
      "\u001b[36mINFO\u001b[0m[0031] Removing the tarball(s) from image volume... \n",
      "\u001b[36mINFO\u001b[0m[0032] Removing k3d-tools node...                   \n",
      "\u001b[36mINFO\u001b[0m[0032] Successfully imported image(s)               \n",
      "\u001b[36mINFO\u001b[0m[0032] Successfully imported 1 image(s) into 1 cluster(s) \n"
     ]
    }
   ],
   "source": [
    "k3d image import my-hello-server --cluster test-cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e94639",
   "metadata": {},
   "source": [
    "## Create a namespace for the application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55dbfaf-3e98-4d69-b67d-7282c71044a2",
   "metadata": {},
   "source": [
    "Before deploying our application, we will create a new *namespace*. It will contain all Kubernetes resources that belong to, or interact with, our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a05b510f-b565-4782-bcc5-8bdd93a6570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/test created\n"
     ]
    }
   ],
   "source": [
    "kubectl create namespace test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75156e7-de08-4990-a36d-07ca0978ed6a",
   "metadata": {},
   "source": [
    "While not stricly necessary, putting each independent application into its own namespace is a good practice. Advantages of this approach include:\n",
    "* If anything is seriously wrong with the Kubernetes resources of an application, its namespace can be deleted, and one can start from scratch without affecting other applications running in the Kubernetes cluster.\n",
    "* One can create a service account that has only the permissions to view, modify, or create resources in one namespace. Such an account can be used for automated interactions with the cluster, e.g., in continuous integration pipelines. Any accidental effects on applications running in other namespaces can then be avoided.\n",
    "* [Resource quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/) can be assigned to namespaces to limit the amount of resources that the applications in a namespace can use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f59035-8fb0-4d9c-b613-b55403752c06",
   "metadata": {},
   "source": [
    "Now we would like to deploy our application in the new namespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b98109",
   "metadata": {},
   "source": [
    "## In Kubernetes, applications run in pods\n",
    "In Kubernetes, the \"smallest deployable units of computing that you can create and manage\" are [*pods*](https://kubernetes.io/docs/concepts/workloads/pods/). A pod is essentially a set of one or more containers which can share some resources, like network and storage. In our case, a single container, which runs the image that we have just built and uploaded to the cluster, is sufficient.\n",
    "\n",
    "Kubernetes resources are usually defined in YAML files, although it is possible to create some types of resources with `kubectl create` directly on the command line.[<sup id=\"fnref:k8s-resources-command-line\">3</sup>](#fn:k8s-resources-command-line) A pod that runs our application, and that lives in the namespace `test`, could be defined like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c92607d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;28;01mapiVersion\u001b[39;00m:\u001b[38;5;250m \u001b[39mv1\n",
      "\u001b[38;5;28;01mkind\u001b[39;00m:\u001b[38;5;250m \u001b[39mPod\n",
      "\u001b[38;5;28;01mmetadata\u001b[39;00m:\n",
      "\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mnamespace\u001b[39;00m:\u001b[38;5;250m \u001b[39mtest\n",
      "\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mname\u001b[39;00m:\u001b[38;5;250m \u001b[39mhello\n",
      "\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mlabels\u001b[39;00m:\n",
      "\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mapp\u001b[39;00m:\u001b[38;5;250m \u001b[39mhello\n",
      "\u001b[38;5;28;01mspec\u001b[39;00m:\n",
      "\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mcontainers\u001b[39;00m:\n",
      "\u001b[38;5;250m  \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mname\u001b[39;00m:\u001b[38;5;250m \u001b[39mhello-server\n",
      "\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mimage\u001b[39;00m:\u001b[38;5;250m \u001b[39mmy-hello-server\n",
      "\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mimagePullPolicy\u001b[39;00m:\u001b[38;5;250m \u001b[39mIfNotPresent\n",
      "\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mports\u001b[39;00m:\n",
      "\u001b[38;5;250m    \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mcontainerPort\u001b[39;00m:\u001b[38;5;250m \u001b[39m8000\n"
     ]
    }
   ],
   "source": [
    "cat k8s-resources/pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34680d9f",
   "metadata": {},
   "source": [
    "Each Kubernetes resource definition has a number of top-level keys:\n",
    "* The `apiVersion` tells what version the resource comes from.\n",
    "* The `kind` tells what kind of resource is defined.\n",
    "* The `metadata` include the name of the resource, the namespace that it belongs to, and labels. Labels can be used for multiple purposes, some of which will will se later.\n",
    "* The `spec` defines the properties of the resource. In the case of a pod, this includes the containers that the pod consists of. Note that we have to set the `imagePullPolicy` to `IfNotPresent` here. Otherwise, Kubernetes would try to pull the image, which does not work unless it has been pushed to a registry that the cluster can access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc263d7",
   "metadata": {},
   "source": [
    "We could now create the pod in the cluster with `kubectl apply -f k8s-resources/pod.yaml`. However, the more common approach is to create Kubernetes resources which control the creation of pods, such as, e.g., [*deployments*](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), [*jobs*](https://kubernetes.io/docs/concepts/workloads/controllers/job/), or [*daemon sets*](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f15f6b",
   "metadata": {},
   "source": [
    "## Define a deployment that controls the pods running an application\n",
    "\n",
    "Here will will use a deployment, which ensures that a certain number of pods of a given type are running in the cluster.[<sup id=\"fnref:replicaset\">4</sup>](#fn:replicaset) Having more than one pod of a certain type, e.g., a web service that answers incoming requests, can have a number of advantages:\n",
    "* Distributing the incoming traffic over multiple pods can be beneficial because a single pod might not be able to handle a sufficient number of simultaneous requests.\n",
    "* It helps to improve the reliability of the system: if a single pod terminates for whatever reason, the other pods can take over request handling immediately.\n",
    "\n",
    "If a pod terminates, e.g., because of a crash in the application or because the node on which it is running is shut down, a new pod is created automatically, possibly on another node. Moreover, deployments can do other useful things concerning the life cycle of pods. For example, updates of the image version or other parts of the pod spec can be done with zero downtime. The deployment ensures that pods are created and destroyed dynamically at a controlled rate.\n",
    "\n",
    "Note that this may not work well if a pod needs a lot of internal state to do its work. Kubernetes works best with stateless applications.[<sup id=\"fnref:statefulset\">5</sup>](#fn:statefulset)\n",
    "\n",
    "Let's see what a deployment looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd97dc4a-4971-4cc6-a888-a4fac8325b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;28;01mapiVersion\u001b[39;00m:\u001b[38;5;250m \u001b[39mapps/v1\n",
      "\u001b[38;5;28;01mkind\u001b[39;00m:\u001b[38;5;250m \u001b[39mDeployment\n",
      "\u001b[38;5;28;01mmetadata\u001b[39;00m:\n",
      "\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mnamespace\u001b[39;00m:\u001b[38;5;250m \u001b[39mtest\n",
      "\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mname\u001b[39;00m:\u001b[38;5;250m \u001b[39mhello\n",
      "\u001b[38;5;28;01mspec\u001b[39;00m:\n",
      "\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mreplicas\u001b[39;00m:\u001b[38;5;250m \u001b[39m3\n",
      "\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mselector\u001b[39;00m:\n",
      "\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mmatchLabels\u001b[39;00m:\n",
      "\u001b[38;5;250m      \u001b[39m\u001b[38;5;28;01mapp\u001b[39;00m:\u001b[38;5;250m \u001b[39mhello\n",
      "\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mtemplate\u001b[39;00m:\n",
      "\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mmetadata\u001b[39;00m:\n",
      "\u001b[38;5;250m      \u001b[39m\u001b[38;5;28;01mlabels\u001b[39;00m:\n",
      "\u001b[38;5;250m        \u001b[39m\u001b[38;5;28;01mapp\u001b[39;00m:\u001b[38;5;250m \u001b[39mhello\n",
      "\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mspec\u001b[39;00m:\n",
      "\u001b[38;5;250m      \u001b[39m\u001b[38;5;28;01mcontainers\u001b[39;00m:\n",
      "\u001b[38;5;250m      \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mname\u001b[39;00m:\u001b[38;5;250m \u001b[39mhello-server\n",
      "\u001b[38;5;250m        \u001b[39m\u001b[38;5;28;01mimage\u001b[39;00m:\u001b[38;5;250m \u001b[39mmy-hello-server\n",
      "\u001b[38;5;250m        \u001b[39m\u001b[38;5;28;01mimagePullPolicy\u001b[39;00m:\u001b[38;5;250m \u001b[39mIfNotPresent\n",
      "\u001b[38;5;250m        \u001b[39m\u001b[38;5;28;01mports\u001b[39;00m:\n",
      "\u001b[38;5;250m        \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mcontainerPort\u001b[39;00m:\u001b[38;5;250m \u001b[39m8000\n"
     ]
    }
   ],
   "source": [
    "cat k8s-resources/deployment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1178a",
   "metadata": {},
   "source": [
    "The `spec` of the deployent describes the pods that it should control:\n",
    "* `replicas: 3` tells Kubernetes that three pods should be running at all times.\n",
    "* The `template` describes what each pod should look like. Note that this object looks a lot like the definition of the plain pod that we saw earlier.\n",
    "* The `selector` describes how the deployment finds the pods that it controls. The `matchLabels` are compared with the `labels` in the `metadata` of all pods for this purpose.\n",
    "\n",
    "To create the deployment in the cluster, we use this command:[<sup id=\"fnref:apply-deployment-updates\">6</sup>](#fn:apply-deployment-updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96314fdd-f224-4e75-bead-6e1e4b6dd10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/hello created\n"
     ]
    }
   ],
   "source": [
    "kubectl apply -f k8s-resources/deployment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119192f",
   "metadata": {},
   "source": [
    "If we look at the list of pods in our namespace now, we get this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd01288-b229-4a74-b444-91f9800b8617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     READY   STATUS              RESTARTS   AGE   IP       NODE                        NOMINATED NODE   READINESS GATES\n",
      "hello-6ddc49b4d4-vzzc8   0/1     ContainerCreating   0          0s    <none>   k3d-test-cluster-server-0   <none>           <none>\n",
      "hello-6ddc49b4d4-5vdwc   0/1     ContainerCreating   0          0s    <none>   k3d-test-cluster-server-0   <none>           <none>\n",
      "hello-6ddc49b4d4-dlm7w   0/1     ContainerCreating   0          0s    <none>   k3d-test-cluster-server-0   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "kubectl -n test get pod -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272fb0e",
   "metadata": {},
   "source": [
    "There are three pods because we set the number of replicas to three in the definition of the deployment. Moreover, all pods have the status `ContainerCreating`, so they are not active yet.\n",
    "\n",
    "Now we can either wait for a few seconds, or use `kubectl rollout status` to wait until all pods are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e87ca991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment \"hello\" rollout to finish: 0 of 3 updated replicas are available...\n",
      "Waiting for deployment \"hello\" rollout to finish: 1 of 3 updated replicas are available...\n",
      "Waiting for deployment \"hello\" rollout to finish: 2 of 3 updated replicas are available...\n",
      "deployment \"hello\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "kubectl -n test rollout status deployment/hello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f1a2c4-71e7-4760-bb20-f16562b7f593",
   "metadata": {},
   "source": [
    "Now all pods are running. We can also see that each pod got its own IP address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6b7ee7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     READY   STATUS    RESTARTS   AGE   IP           NODE                        NOMINATED NODE   READINESS GATES\n",
      "hello-6ddc49b4d4-5vdwc   1/1     Running   0          3s    10.42.0.9    k3d-test-cluster-server-0   <none>           <none>\n",
      "hello-6ddc49b4d4-vzzc8   1/1     Running   0          3s    10.42.0.11   k3d-test-cluster-server-0   <none>           <none>\n",
      "hello-6ddc49b4d4-dlm7w   1/1     Running   0          3s    10.42.0.10   k3d-test-cluster-server-0   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "kubectl -n test get pod -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ae9c64",
   "metadata": {},
   "source": [
    "## Accessing the pods via HTTP within the Kubernetes cluster\n",
    "The IP addresses which Kubernetes assigns to the pods are not reachable from outside the cluster. We will consider how to use a Kubernetes *service* and an *ingress* to make our application accessible for the outside world in the next post.\n",
    "\n",
    "For the time being, we can use those IP addresses to connect to the pods from other pods in the cluster though, as we will show next.\n",
    "\n",
    "To get the IP address of one of the pods, we could look at the output of `kubectl -n test get pod -o wide` above. We could also parse the output with shell commands to assign the IP to a variable. However, `kubectl` also offers other output formats which are easier to process:\n",
    "* `-o json` outputs a JSON object that contains all information about pods.\n",
    "* `-o jsonpath=...` allows to specify a path to the information we need inside the JSON object, and prints just that.\n",
    "\n",
    "By looking for IP addresses in the JSON output (which I will not show here because it is rather lengthy), we conclude that the IP address of the first pod in the list can be obtained like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77216b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.42.0.9\n"
     ]
    }
   ],
   "source": [
    "first_pod_ip=$(kubectl -n test get pods -o jsonpath='{.items[0].status.podIPs[].ip}')\n",
    "echo $first_pod_ip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f1f40-3ad4-4215-92dc-7869ae835e01",
   "metadata": {},
   "source": [
    "To verify that the application can be reached at this address from within the cluster, we create a new pod, which serves as the client that accesses our application. This can be done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dbe8bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"data\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"message\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Hello Frank!\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"info\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"hostname\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"hello-6ddc49b4d4-5vdwc\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "kubectl run --rm --attach -q --image=busybox --restart Never my-test-pod -- wget $first_pod_ip:8000/greet/Frank -q -O - | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d0fd8-32ec-4db9-a854-9e90f8c26b59",
   "metadata": {},
   "source": [
    "We have sucessfully made an HTTP request to our application! Note that the `hostname` in the output is indeed the name of the first pod, whose IP address we have used here.\n",
    "\n",
    "The options to `kubectl run` have the following meaning:\n",
    "* `--rm` ensures that the pod is deleted after it exits, such that it no longer occupies resources in the cluster.\n",
    "* `--attach` attaches to the process in the pod, such that we can see its output in the terminal.\n",
    "* `-q` suppresses output from `kubectl` - we are only interested in output from `wget`.\n",
    "* `--image=busybox` sets the image that the single container in the pod will use. All we need is a way to make HTTP requests from the pod, so we will use the `busybox` image, which contains a variant of `wget`.\n",
    "* `--restart Never` prevents that Kubernetes restarts the pod after termination.[<sup id=\"fnref:restart-never\">7</sup>](#fn:restart-never)\n",
    "* `my-test-pod` is the name of the pod. This can be any name that is not taken yet in the namespace. Note that we don't use a namespace argument here, so our pod will be created in the `default` namespace.\n",
    "\n",
    "Using pod IPs to access our application has a number of downsides though. This post is already a bit long though, so we will discuss them and look at the solutions that Kubernetes provides for this issue, namely, services and ingresses, in a future post.\n",
    "\n",
    "## Deleting the cluster\n",
    "For the time being, we are done with our experiments. We could stop the cluster now with `k3d cluster stop` and restart it later with `k3d cluster start`. Everything in the cluster is easy to restore though, so we will delete it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d60a6f7e-255f-420c-a396-03343fd9b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0000] Deleting cluster 'test-cluster'              \n",
      "\u001b[36mINFO\u001b[0m[0002] Deleting cluster network 'k3d-test-cluster'  \n",
      "\u001b[36mINFO\u001b[0m[0003] Deleting 2 attached volumes...               \n",
      "\u001b[33mWARN\u001b[0m[0003] Failed to delete volume 'k3d-test-cluster-images' of cluster 'test-cluster': failed to find volume 'k3d-test-cluster-images': Error: No such volume: k3d-test-cluster-images -> Try to delete it manually \n",
      "\u001b[36mINFO\u001b[0m[0003] Removing cluster details from default kubeconfig... \n",
      "\u001b[36mINFO\u001b[0m[0003] Removing standalone kubeconfig file (if there is one)... \n",
      "\u001b[36mINFO\u001b[0m[0003] Successfully deleted cluster test-cluster!   \n"
     ]
    }
   ],
   "source": [
    "k3d cluster delete test-cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0168f08-e630-4de0-9c93-9261f88e3ded",
   "metadata": {},
   "source": [
    "Note that all resources are removed despite the warning about the failed deletion of the Docker volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66612f92-8086-4a87-9624-3ce24d64774c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this post, we created a local single-node Kubernetes cluster with k3d. Moreover, we deployed a small self-made application in the cluster, and connected to this application via HTTP from within the cluster.\n",
    "\n",
    "In the next post, we will investigate how to access the application from outside the cluster.\n",
    "\n",
    "---\n",
    "\n",
    "1. <span id=\"fn:k3d-multiple-nodes\">If</span> we wanted more server or worker nodes, we could specify the desired numbers with the `--agents` and `--servers` options to `k3d cluster create`, respectively. [&#8617;](#fnref:k3d-multiple-nodes)\n",
    "\n",
    "2. <span id=\"fn:local-image-caveat\">There</span> is one thing to keep in mind though: by default, Kubernetes will try to pull the image from a registry even if it is available on the node running the application, and therefore, the application will fail to run. We will see in a minute how this can be prevented by setting a suitable `imagePullPolicy` for the pods using the image. [&#8617;](#fnref:local-image-caveat)\n",
    "\n",
    "3. <span id=\"fn:k8s-resources-command-line\">We</span> have already created a Kubernetes resource on the command line with `kubectl create`: the namespace for our application, which we have created with `kubectl create namespace test`. We could also have achieved this using a YAML document with the content below: [&#8617;](#fnref:k8s-resources-command-line)\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: test\n",
    "```\n",
    "\n",
    "4. <span id=\"fn:replicaset\">To</span> be precise, the deployment does not control the number of pods directly. It achieves this with a [*replica set*](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/) which is a lower-level object that has controlling the number of pods as its sole purpose. The deployment adds other useful functionality, such as, e.g., updating image versions. [&#8617;](#fnref:replicaset)\n",
    "\n",
    "5. <span id=\"fn:statefulset\">It</span> is possible to work with stateful applications in Kubernetes though. [*Stateful sets*](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) cah help with that. [&#8617;](#fnref:statefulset)\n",
    "\n",
    "6. <span id=\"fn:apply-deployment-updates\">Note</span> that `kubectl apply` is not only useful for creating deployments and other resources. The same command can be used to make changes to the resource. A common example would be to modify a deployment such that the image version is updated. [&#8617;](#fnref:apply-deployment-updates)\n",
    "\n",
    "7. <span id=\"fn:restart-never\">Restarting terminated pods is the default behavior because most applications running in Kubernetes clusters are services which should always be up.</span> [&#8617;](#fnref:restart-never)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "nikola": {
   "date": "2022-07-12 20:01:51 UTC",
   "slug": "../../blog/2022/07/12/set-up-local-k8s-cluster-and-deploy-self-made-microservice",
   "tags": "docker,kubernetes",
   "title": "How to set up a local Kubernetes cluster and deploy a self-made microservice in less than 10 minutes"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
